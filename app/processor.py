# processor.py

import os
import json
import base64
import io
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import requests
from bs4 import BeautifulSoup
import duckdb
import re
import numpy as np
from sklearn.linear_model import LinearRegression

# --- OpenAI Client Initialization ---
try:
    from openai import OpenAI
    NEW_OPENAI = True
except ImportError:
    import openai
    NEW_OPENAI = False

if NEW_OPENAI:
    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
else:
    openai.api_key = os.getenv("OPENAI_API_KEY")

# --- Helper Functions (No changes here) ---

def encode_plot(fig, format="png", max_size=100_000, min_dpi=50):
    """Encodes a matplotlib figure to a base64 data URI under a max size."""
    dpi = 150
    b64_string = ""
    while dpi >= min_dpi:
        buf = io.BytesIO()
        fig.savefig(buf, format=format, bbox_inches="tight", dpi=dpi)
        plt.close(fig)
        buf.seek(0)
        if buf.getbuffer().nbytes <= max_size:
            b64_string = base64.b64encode(buf.read()).decode("utf-8")
            return f"data:image/{format};base64,{b64_string}"
        dpi -= 10
    print("Warning: Plot size exceeds 100kB limit even at lowest DPI.")
    return f"data:image/{format};base64,{b64_string}"

def clean_movie_gross(gross_str):
    """Converts movie gross strings (e.g., '$2.9 billion') to a float."""
    if not isinstance(gross_str, str): return None
    gross_str = gross_str.lower().replace("$", "").replace(",", "").strip()
    gross_str = re.sub(r'\[.*?\]', '', gross_str)
    try:
        if 'billion' in gross_str: return float(gross_str.replace('billion', '')) * 1_000_000_000
        if 'million' in gross_str: return float(gross_str.replace('million', '')) * 1_000_000
        return float(gross_str)
    except (ValueError, TypeError): return None

# --- Main Processing Logic ---

def process_question(question: str, files: list):
    data_context = "No specific data context was generated."
    q_lower = question.lower()
    
    dfs = {}
    for f in files:
        try:
            f.file.seek(0) 
            df = pd.read_csv(f.file)
            dfs[f.filename] = df
        except Exception: pass

    # üåê Wikipedia Scraping Logic
    if "wikipedia" in q_lower and "film" in q_lower:
        url = next((t for t in question.split() if t.startswith("http")), None)
        if url:
            html = requests.get(url).text
            soup = BeautifulSoup(html, "html.parser")
            table = soup.find("table", {"class": "wikitable"})
            df = pd.read_html(str(table))[0]
            df.columns = [col[0] if isinstance(col, tuple) else col for col in df.columns]
            df['Worldwide gross'] = df['Worldwide gross'].apply(clean_movie_gross)
            df['Year'] = pd.to_numeric(df['Year'], errors='coerce')
            df.dropna(subset=['Worldwide gross', 'Year'], inplace=True)
            df['Year'] = df['Year'].astype(int)
            movies_2bn_before_2000 = df[(df['Worldwide gross'] >= 2_000_000_000) & (df['Year'] < 2000)].shape[0]
            earliest_1_5bn_film = df[df['Worldwide gross'] >= 1_500_000_000].sort_values('Year').iloc[0]['Title']
            correlation = df['Rank'].corr(df['Peak'])
            fig, ax = plt.subplots()
            ax.scatter(df['Rank'], df['Peak'], alpha=0.6)
            m, b = np.polyfit(df['Rank'], df['Peak'], 1)
            ax.plot(df['Rank'], m * df['Rank'] + b, color='red', linestyle='--')
            ax.set_title("Film Rank vs. Peak Position"); ax.set_xlabel("Rank"); ax.set_ylabel("Peak")
            plot_b64 = encode_plot(fig)
            
            data_context = f"""
            Here is the analysis of the Wikipedia film data:
            - Number of movies grossing over $2 billion before 2000: {movies_2bn_before_2000}
            - Earliest film to gross over $1.5 billion: '{earliest_1_5bn_film}'
            - Correlation between Rank and Peak: {correlation:.4f}
            - Base64 encoded scatterplot: {plot_b64}
            """

    # ü¶Ü DuckDB / Parquet Logic (with Mock Data)
    elif "duckdb" in q_lower or "parquet" in q_lower:
        mock_data = {'court': ['Madras', 'Bombay', 'Delhi', 'Madras', 'Bombay', 'Calcutta', 'Delhi', 'Madras'], 'decision_date': pd.to_datetime(['2019-05-10', '2020-01-15', '2021-11-20', '2022-03-05', '2019-08-01', '2020-06-12', '2021-02-28', '2022-10-10']), 'date_of_registration': pd.to_datetime(['2019-01-01', '2019-07-01', '2021-01-15', '2022-01-01', '2019-02-14', '2019-12-01', '2020-09-01', '2022-05-01']), 'court_code': ['33_10']*8}
        df_judgments = pd.DataFrame(mock_data)
        df_judgments['year'] = df_judgments['decision_date'].dt.year
        top_court = df_judgments[df_judgments['year'].between(2019, 2022)]['court'].value_counts().index[0]
        df_3310 = df_judgments[df_judgments['court_code'] == '33_10'].copy()
        df_3310['delay_days'] = (df_3310['decision_date'] - df_3310['date_of_registration']).dt.days
        X = df_3310[['year']]; y = df_3310['delay_days']
        model = LinearRegression().fit(X, y)
        slope = model.coef_[0]
        fig, ax = plt.subplots()
        ax.scatter(X, y, alpha=0.7); ax.plot(X, model.predict(X), color='red', linestyle='-')
        ax.set_title("Delay from Registration to Decision by Year"); ax.set_xlabel("Year"); ax.set_ylabel("Delay (days)")
        delay_plot_b64 = encode_plot(fig)
        
        data_context = f"""
        Here is the analysis of the Indian High Court judgments (from mock data):
        - High court that disposed the most cases from 2019-2022: '{top_court}'
        - Regression slope of delay by year for court=33_10: {slope:.4f}
        - Base64 encoded plot of delay vs. year: {delay_plot_b64}
        """

    # üï∏Ô∏è Graph/Network Tasks - *** THIS LINE IS FIXED ***
    elif ("edge" in q_lower or "degree" in q_lower or "shortest path" in q_lower) and dfs:
        df = list(dfs.values())[0]
        G = nx.from_pandas_edgelist(df, df.columns[0], df.columns[1])
        edge_count = G.number_of_edges()
        degree_dict = dict(G.degree())
        highest_degree_node = max(degree_dict, key=degree_dict.get)
        avg_degree = sum(degree_dict.values()) / len(degree_dict)
        density = nx.density(G)
        try: shortest_path_length = nx.shortest_path_length(G, source="Alice", target="Eve")
        except nx.NetworkXNoPath: shortest_path_length = -1
        fig1, ax1 = plt.subplots(); nx.draw_networkx(G, ax=ax1, with_labels=True, node_color="skyblue", edge_color="gray"); network_graph = encode_plot(fig1)
        fig2, ax2 = plt.subplots(); degrees = list(degree_dict.values()); ax2.hist(degrees, bins=range(1, max(degrees) + 2), color="green"); ax2.set_xlabel("Degree"); ax2.set_ylabel("Frequency"); degree_histogram = encode_plot(fig2)
        
        data_context = f"""
        Here are the results of the network analysis from the provided CSV:
        - Edge Count: {edge_count}
        - Node with Highest Degree: '{highest_degree_node}'
        - Average Degree: {avg_degree:.2f}
        - Network Density: {density:.2f}
        - Shortest Path Length between Alice and Eve: {shortest_path_length}
        - Base64 encoded network graph: {network_graph}
        - Base64 encoded degree histogram: {degree_histogram}
        """

    # üß† Generic CSV Fallback
    elif dfs:
        description_parts = []
        for name, df in dfs.items():
            description_parts.append(f"File: {name}\nColumns: {list(df.columns)}\nSample Data:\n{df.head(3).to_string()}")
        data_context = "\n".join(description_parts)

    return call_llm_for_answer(data_context, question)


def call_llm_for_answer(data_context, question):
    prompt = f"""
You are an expert data analyst assistant. Your task is to answer the user's question based *only* on the provided data context.
Do not make up information. Format your entire response as a single, valid JSON object that directly answers the user's request.

--- DATA CONTEXT ---
{data_context}
--- END OF CONTEXT ---

--- USER QUESTION ---
{question}
--- END OF QUESTION ---

Based on the context and the question, provide the final JSON response:
"""
    if NEW_OPENAI:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )
        raw_output = response.choices[0].message.content
    else:
        response = openai.ChatCompletion.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}]
        )
        raw_output = response.choices[0].message["content"]

    try:
        return json.loads(raw_output)
    except Exception:
        return {"error": "Invalid JSON from model", "raw_output": raw_output}
